# STARDUST Architecture Overview

This document provides an overview of the STARDUST framework's architecture, its theoretical underpinnings in Spectrum-Based Fault Localization (SBFL), and practical examples of its components.

## Theoretical Background: Spectrum-Based Fault Localization (SBFL)

Spectrum-based fault localization (SBFL) techniques are used to identify the location of faults (bugs) in a program. The core idea is to analyze the program's "spectrum" â€“ information about which parts of the code were executed during various test runs.

SBFL operates on the following general principles:

1.  **System Model:** It assumes a system composed of multiple discrete components (e.g., lines of code, methods, classes).
2.  **Execution Traces:** The system's execution is tracked for a set of test cases. Each execution is recorded as a "trace."
3.  **Trace Classification:** Each trace is classified as either:
    *   **Passing:** The execution completed without encountering an error.
    *   **Failing:** The execution resulted in an error.
4.  **Component Involvement:** For each trace, the framework records which components were "involved" (i.e., executed) and which were not.
5.  **Suspiciousness Ranking:** SBFL algorithms use statistical formulas (often similarity coefficients) to calculate a "suspiciousness score" for each component. Components that are frequently involved in failing traces and infrequently in passing traces receive higher scores. The components are then ranked by these scores, with the highest-ranked components being the most likely candidates for the fault location.

The STARDUST framework implements various SBFL algorithms and provides tools to conduct experiments using this approach.

## STARDUST Framework Architecture

The STARDUST framework is designed to facilitate experiments with SBFL techniques. Its architecture can be understood through its main components:

### 1. Traces

Traces are fundamental to SBFL. In STARDUST, traces represent the execution path of the program for a given test case, detailing which components were covered.

*   **Representation:** The `fk.stardust.traces` namespace contains classes for representing program traces in an object-oriented manner. This includes information about covered components (e.g., line numbers, method names) and whether the execution was a pass or a fail.
*   **Data Format:** While the framework is designed to be extensible, it currently has strong support for traces generated by the [Cobertura](http://cobertura.github.io/cobertura/) coverage tool. Cobertura output files provide the necessary information about line coverage for each test.
*   **Example:** A trace might indicate that for `testCase1` (failed), lines 10, 12, 15-20 of `MyClass.java` were executed. For `testCase2` (passed), lines 10, 12, 22-25 of `MyClass.java` were executed.

### 2. Spectra Providers

Spectra providers are responsible for loading trace data from a specific storage format into the framework's internal representation.

*   **Functionality:** The `fk.stardust.provider` namespace handles the loading of traces.
*   **Current Implementation:** The primary provider is `CoberturaProvider`, designed to parse Cobertura XML coverage reports.
*   **Extensibility:** This component can be extended to support other coverage tools or trace formats.
*   **Example:**
    ```java
    // (Simplified from actual usage)
    ISpectraProvider<String> provider = new CoberturaProvider("path/to/cobertura/xml/files");
    Spectra<String> programSpectra = provider.loadSpectra();
    ```
    This `programSpectra` object would then contain all the trace information (passing/failing status and component coverage for each test case) needed by the localizers.

### 3. Fault Localizers (SBFL Algorithms)

Fault localizers implement the core SBFL logic. They take the program spectra as input and produce a ranked list of components based on their calculated suspiciousness scores.

*   **Implementation:** The `fk.stardust.localizer` namespace, particularly `fk.stardust.localizer.sbfl`, contains implementations of various SBFL algorithms (similarity coefficients). The research papers indicate around 30 different SBFL metrics are implemented (e.g., Tarantula, Ochiai, Jaccard).
*   **Process:** Each localizer applies its specific formula to the counts of how many passing/failing tests execute each component.
*   **Output:** The output is a `Ranking` object, where components (e.g., line numbers) are ordered from most to least suspicious.
*   **Example:**
    ```java
    // (Simplified from actual usage)
    IFaultLocalizer<String> tarantulaLocalizer = new Tarantula<>(); // Or any other implemented localizer
    Ranking<String> ranking = tarantulaLocalizer.localize(programSpectra);

    // The 'ranking' object now holds components ranked by their suspiciousness.
    // For instance, it might indicate "MyClass.java:15" is the most suspicious line.
    System.out.println("Most suspicious component: " + ranking.getBestRanking().get(0).getElement());
    ```

### 4. Experiments and Evaluation

The framework is built to conduct and evaluate SBFL experiments, often on benchmark datasets.

*   **Purpose:** The `fk.stardust.evaluation` namespace contains classes used to run experiments, manage datasets, and evaluate the effectiveness of different localizers.
*   **Datasets:** The framework has been notably used with the [iBugs bug data set](https://www.st.cs.uni-saarland.de/ibugs/), which provides a collection of real-world bugs and their corresponding test cases.
*   **Process:**
    1.  **Setup:** An experiment involves selecting a buggy program version from a dataset.
    2.  **Trace Collection:** Coverage data (traces) is generated/collected for the program's test suite (both passing and failing tests related to the bug).
    3.  **Localization:** One or more SBFL localizers are applied to the collected spectra.
    4.  **Evaluation:** The resulting rankings are evaluated. A common metric is "EXAM score," which measures how many components (e.g., lines of code) an engineer would need to inspect in the ranked list before finding the actual faulty component.
*   **Example (Conceptual):**
    ```
    // Conceptual flow of an experiment
    BugInstance bug = iBugsDataset.getBug("AspectJ-1");
    Spectra<String> spectraForBug = collectTracesForBug(bug); // Using a SpectraProvider

    IFaultLocalizer<String> ochiaiLocalizer = new Ochiai<>();
    Ranking<String> ochiaiRanking = ochiaiLocalizer.localize(spectraForBug);

    double examScore = calculateExamScore(ochiaiRanking, bug.getActualFaultyLines());
    System.out.println("EXAM Score for Ochiai on " + bug.getName() + ": " + examScore);
    ```

## Practical Interaction of Components

A typical workflow using STARDUST involves:

1.  **Data Preparation:**
    *   Obtain a software project with a known bug and a test suite capable of triggering that bug (and also passing tests).
    *   Instrument the code and run the test suite to generate coverage reports (e.g., Cobertura XML files). One report for failing tests, and potentially multiple for passing tests, or a combined report that distinguishes them.

2.  **Loading Spectra:**
    *   Use a `SpectraProvider` (like `CoberturaProvider`) to load these coverage reports into a `Spectra` object. This object aggregates all trace information.

3.  **Applying a Fault Localizer:**
    *   Instantiate one or more `IFaultLocalizer` implementations (e.g., `Tarantula`, `Ochiai`).
    *   Pass the `Spectra` object to the `localize()` method of the chosen localizer.

4.  **Analyzing the Ranking:**
    *   The `localize()` method returns a `Ranking` object. This object can be inspected to see the list of program components (lines, methods, etc.) ordered by their suspiciousness.
    *   The ranking can be saved to a file or used for further analysis (e.g., calculating how much code needs to be inspected to find the bug).

```java
// Simplified end-to-end example from README.md
// Assumes Cobertura XML files are available and configured for the provider

// 1. & 2. Load Spectra (Data Preparation implicitly done before)
ISpectraProvider<String> provider = new CoberturaProvider(); // Configuration details omitted
Spectra<String> programSpectra = provider.loadSpectra();

// 3. Apply a Fault Localizer
IFaultLocalizer<String> tarantula = new Tarantula<>();
Ranking<String> ranking = tarantula.localize(programSpectra);

// 4. Save/Analyze Ranking
ranking.save("resulting-ranking.txt"); // Saves the ranked list of suspicious components
System.out.println("Top suspicious element: " + ranking.getBestRanking().get(0).getElement());
```

This architecture allows researchers and developers to easily experiment with different SBFL algorithms, integrate various sources of execution trace data, and evaluate localization performance on different software projects and bug datasets.
